<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Prompt Engineering</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha512-NhSC1YmyruXifcj/KFRWoC561YpHpc5Jtzgvbuzx5VozKpWvQ+4nXhPdFgmx8xqexRcpAglTj9sIBWINXa8x5w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<link rel="stylesheet" href="./static/style.css" type="text/css">
<link rel="stylesheet" href="./static/pygments.css" type="text/css">
<!-- <link rel="shortcut icon" href="./static/icon.ico" /> -->
</head>
<body>
<header>
  <div class="title"><span>Chapter 3: Prompt Engineering</span></div>
  <input type="checkbox" id="toggle">
  <label class="hamburger" for="toggle"><span></span><span></span><span></span></label>
  <ul class="menu">
    <li><a href="./index.html">Front Cover</a></li>
    <li><a href="./table-of-contents.html">Table of Contents</a></li>
    <li><a href="https://github.com/vladris/llm-book/issues/new">üì£ Feedback</a></li>
    <li><a href="https://tinyletter.com/vladris">‚úâÔ∏è Subscribe for updates</a></li>
  </ul>
</header>
<article>
<h1>Prompt Engineering</h1>

<p><img src="images/03/cover.png" alt="Prompt Engineering"></p>

<p>In this chapter:</p>

<ul>
<li>Designing powerful prompts.</li>
<li>Using templates for prompts.</li>
<li>Selecting between multiple templates.</li>
<li>Chaining prompts to perform complex tasks.</li>
</ul>

<p>This chapter is all about prompts, the fundamental way we interact with large
language models. Prompts are key components of any solution built around these
models, so we need to have a solid understanding of how to leverage them to the
maximum.</p>

<p>We‚Äôll start with <em>prompt design</em> and <em>tuning</em> and see how small tweaks to the
prompt can elicit very different responses from a large language model. We‚Äôll
iterate over a simple example and look at some tips on what we should include in
a prompt.</p>

<p>If we‚Äôll spend so much time coming up with the perfect prompt, we‚Äôll probably
want to reuse it. We‚Äôll see how we can do this via <em>prompt templating</em>. We‚Äôll
implement a generic function that will enable us to specify prompt templates as
JSON files, then load these to fill in the prompts we send to the model.</p>

<p>In many scenarios, we have multiple prompt templates we can use, and we need to
solve the problem of which one to pick based on what the user wants. Luckily,
understanding user intent is something large language models are great at. We‚Äôll
implement <em>prompt selection</em> and automate picking the right prompt.</p>

<p>Most non-trivial tasks can‚Äôt be accomplished through a single prompt ‚Äì we‚Äôll
have to divide and conquer. We can complete complex tasks with multiple
(connected) prompts, something known as <em>prompt chaining</em>. We‚Äôll cover the
arguably simplest type of chaining ‚Äì maintaining a chat history, then we‚Äôll look
at a more complex scenario involving a multi-step workflow. Let‚Äôs start with the
basics though: design.</p>

<h2 id="prompt-design-&amp;-tuning">Prompt design &amp; tuning</h2>

<p>In chapter 1 we briefly touched on the emerging discipline of <em>prompt
engineering</em> and defined it as the process of creating and optimizing natural
language prompts for large language models. Let‚Äôs see why prompts are such a key
piece of integrating these models into software solutions.</p>

<p>We‚Äôll start with a simple scenario: we have some content we‚Äôd like summarized.
You can use <a href="https://chat.openai.com/">https://chat.openai.com/</a> for the examples in this section, or the
OpenAI playground (<a href="https://platform.openai.com/playground/">https://platform.openai.com/playground/</a>). Listing 3.1 shows
the content ‚Äì a business plan for a new minimalist phone.</p>
<div class="highlight"><pre><span></span>Our company aims to disrupt the smartphone market by introducing a minimalist
phone that focuses on the essentials. We believe that in a world where
technology is becoming increasingly complex, there is a growing demand for a
device that simplifies our lives and allows us to be more present in the moment.

Our minimalist phone will have a simple and sleek design, with a monochrome
display and limited features. It will focus on the core functionalities of a
phone, such as calling, texting, and taking photos, while eliminating
unnecessary distractions and time-wasting apps. By doing so, our phone will
promote mindfulness and productivity, while also reducing screen time and
digital overload.

In terms of pricing, we plan to position our minimalist phone as a premium
product that is worth the investment. We believe that our target audience, which
consists of busy professionals, digital minimalists, and people who value
simplicity and mindfulness, will be willing to pay a premium for a device that
aligns with their values and lifestyle.

To market our minimalist phone, we will leverage social media and influencer
marketing, as well as targeted advertising campaigns. We will also partner with
like-minded brands and organizations that share our vision of simplifying and
improving people&#39;s lives. Finally, we will prioritize customer satisfaction and
offer exceptional customer service and support, as well as regular updates and
improvements to the phone&#39;s software and functionality.

Overall, we are confident that our minimalist phone will revolutionize the
smartphone industry and offer a refreshing alternative to the current state of
technology. We look forward to launching our product and making a positive
impact on people&#39;s lives.
</pre></div>

<p><em>Listing 3.1: A business plan for a minimalist phone (AI generated).</em></p>

<p>We‚Äôd like to get this summarized into a few short bullet points, which we can
surface in our UI. We prompt the model with a simple prompt shown in listing 3.2
(business plan omitted).</p>
<div class="highlight"><pre><span></span>Summarize this business plan &quot;&lt;business plan from listing 3.1 goes here&gt;&quot;
</pre></div>

<p><em>Listing 3.2: Prompt to summarize the business plan.</em></p>

<p>We might get back something like listing 3.3 (again, large language models are
non-deterministic so you might get a different response).</p>
<div class="highlight"><pre><span></span>The business plan outlines a strategy to disrupt the smartphone market by
introducing a minimalist phone that focuses on the essentials, with a simple and
sleek design, limited features, and a monochrome display. The phone will be
positioned as a premium product, targeting busy professionals, digital
minimalists, and people who value simplicity and mindfulness. The company plans
to leverage social media, influencer marketing, and partnerships with
like-minded brands to market the phone. The company will prioritize customer
satisfaction, offer regular updates and improvements, and aims to make a
positive impact on people&#39;s lives.
</pre></div>

<p><em>Listing 3.3: Summary of business plan.</em></p>

<p>This is a good summary, but not quite what we want ‚Äì let‚Äôs explicitly ask for bullet points. Listing 3.4 shows the <em>refined prompt</em>.</p>
<div class="highlight"><pre><span></span>Summarize this business plan &quot;&lt;business plan from listing 3.1 goes here&gt;&quot; in a
few short bullet points
</pre></div>

<p><em>Listing 3.4: Refined prompt to summarize the business plan.</em></p>

<p>Since we introduced the term, let‚Äôs also define it:</p>

<blockquote>
<p><strong>Definition</strong>: A <em>refined prompt</em> is a prompt revised and optimized through
multiple iterations to elicit the most accurate and desirable responses from a
large language model. This typically involves tuning the language, syntax,
structure, and context of the prompt to make it more specific and effective in
achieving the desired outcome.</p>
</blockquote>

<p>Refined prompts are crucial for improving the performance of large language
models across all applications. In our case, the refined prompt gives us the
output in listing 3.5.</p>
<div class="highlight"><pre><span></span>* Company aims to disrupt smartphone market with minimalist phone
* Phone will have simple and sleek design, with limited features
* Focus on core functionalities of calling, texting, and photos
* Target audience consists of busy professionals, digital minimalists, and those
  who value simplicity and mindfulness
* Phone will be positioned as a premium product worth investment
* Marketing will leverage social media, influencer marketing, and targeted
  advertising
* Partnership with like-minded brands and organizations
* Priority on customer satisfaction and support with regular updates and
  improvements
* Goal is to revolutionize smartphone industry and make positive impact on
  people&#39;s lives
</pre></div>

<p><em>Listing 3.5: Summary of business plan.</em></p>

<p>This is closer, but still too many bullet points and too many words ‚Äì our UI can‚Äôt fit all of this. Let‚Äôs try again with listing 3.6.</p>
<div class="highlight"><pre><span></span>Summarize this business plan &quot;&lt;business plan from listing 3.1 goes here&gt;&quot; in no
more than 5 bullet points, at most 7 words each
</pre></div>

<p><em>Listing 3.6: More detailed prompt.</em></p>

<p>This gets us the output in listing 3.7.</p>
<div class="highlight"><pre><span></span>* Minimalist phone focusing on essentials
* Promotes mindfulness and productivity
* Premium product for busy professionals
* Leverage social media and partnerships
* Revolutionize smartphone industry with simplicity
</pre></div>

<p><em>Listing 3.7: Summary of business plan.</em></p>

<p>This is close to what we want. This simple example was an exercise in <em>prompt
design</em> and <em>tuning</em>.</p>

<blockquote>
<p><strong>Definition</strong>: <em>Prompt design</em> is the process of constructing a high-quality
input sequence, or prompt, that will guide the model to generate a desired
output.</p>
</blockquote>

<p>The prompt typically consists of a few words, phrases, or sentences that provide
context and direction for the model to generate text that aligns with the user&#39;s
intention. Effective prompt design significantly improves the quality and
relevance of the generated text.</p>

<p>Prompt design involves carefully selecting and arranging the words and phrases
in the prompt to guide the model toward the desired output, while avoiding
ambiguity or confusion.</p>

<p>We are still discovering the best, most effective prompt recipes, but a starting
point for a good prompt includes telling the model:</p>

<ul>
<li><strong>Who to act like</strong> - This can be ‚Äúyou are an AI assistant‚Äù, or ‚Äúrespond as a
high-school teacher‚Äù, or ‚Äúact like Stephen King‚Äù.</li>
<li><strong>Additional context</strong> ‚Äì We‚Äôll cover this in a lot more depth later on when we
discuss memory in chapter 5. For now, think of this as any additional
information that helps clarify the ask or provides data that can be used when
generating the answer.</li>
<li><strong>Who the audience is</strong> ‚Äì You can tweak the response by being specific about
the audience (e.g., ‚Äúexplain like I‚Äôm 5 years old‚Äù).</li>
<li><strong>How the output should look like</strong> ‚Äì You can be as descriptive as needed.
*‚ÄúShort summary‚Äù, ‚Äú500 word article‚Äù, or ‚Äú5 bullet points no more than 7 words
*each‚Äù.</li>
<li><strong>How to go about generating the output</strong> ‚Äì Additional instructions for the
*large language model, like ‚Äúthink step by step‚Äù.</li>
<li><strong>Examples</strong> ‚Äì It helps to give examples; we‚Äôll talk more about this in the
next chapter. Examples are good way of showing the large language model what
you expect.</li>
<li><strong>Proper syntax</strong> ‚Äì Using the correct capitalization, grammar, and punctuation
*helps (as it should more closely match data the model has been trained on).</li>
</ul>

<p>For example, if we‚Äôre talking about integration within other systems, we care
about the shape of the output. Instead of asking <code>summarize this business plan</code>,
we are specific about how we want the format (<code>5 bullet points, no more than 7
words each</code>).</p>

<p>Of course, for complex scenarios, you won‚Äôt get the prompt right in one go.
You‚Äôll have to refine it several times. This iterative process is called <em>prompt
tuning</em>.</p>

<blockquote>
<p><strong>Definition</strong>: <em>Prompt tuning</em> refers to the iterative process of refining
and adjusting prompts used with large language models. In prompt tuning, the
goal is to create prompts that are optimized to produce the desired output from
the model.</p>
</blockquote>

<p>People are getting more and more interesting outputs out of large language
models by coming up with intricate prompts. These highly tuned prompts are
colloquially called <em>superprompts</em>.</p>

<blockquote>
<p><strong>Sidebar: Superprompt example ‚Äì interview practice</strong><sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup></p>

<p>An example of one of this type of ‚Äúmagical‚Äù prompt is a prompt that enables
you to practice interviewing with ChatGPT. In the ChatGPT UI, you can start
with:</p>

<p><code>I want you to act as an interviewer. I will be the candidate and you will ask
me the interview questions for the {{position}} position. I want you to only
reply as the interviewer. Do not write all the conversation at once. I want you
to only do the interview with me. Ask me the questions and wait for my answers.
Do not write explanations. Ask me the questions one by one like an interviewer
does and wait for my answers. My first sentence is &quot;Hi&quot;.</code></p>

<p>Replace <code>{{position}}</code> with the actual position you want to practice for, then
you can use ChatGPT to help you prepare!</p>

<p>Note the detailed description of how the model should reply and what it should
not output. This prompt was likely created with a lot of tuning, starting from a
simpler ask then iterating as the model replied in unrealistic ways. For
example, the ‚Äúdo not write all the conversation at once‚Äù probably ended up there
as without it the large language model would imagine a whole interview exchange
and respond with it in one go.</p>
</blockquote>

<p>A somewhat magical phrase that seems to help in many prompts is ‚Äúthink step by
step‚Äù or variations of it, like ‚Äúthink about this logically‚Äù. This is called
<em>chain-of-thought</em>, explicitly asking the large language model to list the steps
it takes to arrive at a result rather than just producing the result.</p>

<h3>Chain-of-thought prompts</h3>

<p>For example, for more complex questions that involve multiple steps, the model
might offer a wrong response without additional priming. The prompt in listing
3.8, when run against <code>text-davinci-003</code>, gets the wrong response.</p>
<div class="highlight"><pre><span></span>In a bouquet of 12 flowers, half are roses. Half of the roses are red. How many
red roses are in the bouquet?
</pre></div>

<p><em>Listing 3.8: Multiple-step problem prompt.</em></p>

<p>The response is <code>6 roses</code>, which is wrong. <code>gpt-3.5-turbo</code> seems to answer this
correctly, but you get the point ‚Äì more convoluted questions could elicit a
wrong response. Interestingly enough, if we enhance our prompt with ‚Äúthink step
by step‚Äù as in listing 3.9, the model produces the correct response.</p>
<div class="highlight"><pre><span></span>In a bouquet of 12 flowers, half are roses. Half of the roses are red. How many
red roses are in the bouquet? Think step by step.
</pre></div>

<p><em>Listing 3.9: Chain-of-thought prompt.</em></p>

<p>Now we get a more detailed, step-by-step answer, shown in listing 3.10.</p>
<div class="highlight"><pre><span></span>Step 1: Half of 12 flowers is 6. 
Step 2: Half of 6 roses is 3.
Answer: There are 3 red roses in the bouquet.
</pre></div>

<p><em>Listing 3.10: Chain-of-thought prompt response.</em></p>

<p>Turns out the model could generate the correct response all along, but it needed
additional guidance. In some cases, simply adding ‚Äúthink step by step‚Äù or
similar phrases is enough. In other cases, including an example of how this
step-by-step reasoning looks like in the prompt helps. See Listing 3.11 for an
example of this.</p>
<div class="highlight"><pre><span></span>Q: In a bouquet of 24 flowers, half are roses. A third of the roses are red. How
many red roses are in the bouquet?

A: The bouquet has 24 flowers, so half of them being roses means there are 12
roses. A third of the roses are red out of 12 rose, so there are 4 red roses in
the bouquet.

Q: In a bouquet of 12 flowers, half are roses. Half of the roses are red. How
may red roses are in the bouquet?

A:
</pre></div>

<p><em>Listing 3.11: Chain-of-thought prompt with examples.</em></p>

<p>When using this prompt with <code>text-davinci-003</code>, the response mimics the example
and arrives at the correct answer.</p>

<blockquote>
<p><strong>Definition</strong>: <em>Chain-of-thought prompting</em> is a technique that can be used
to improve the reasoning and accuracy performance of large language models by
providing rationales for a given word or phrase. It improves the reasoning
ability of large language models by prompting them to generate a series of
intermediate steps that lead to the final answer of a multi-step problem.</p>
</blockquote>

<p>Chain-of-though explicitly encourages the large language model to generate
intermediate rationales for solving a problem, either through a phrase like
‚Äúthink step by step‚Äù or by providing a series of reasoning steps in a
demonstration that is part of the prompt.</p>

<p>In general, chain-of-thought prompting can elicit better reasoning from large
language models on logic, math, and symbolic reasoning tasks<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup>. Keep this in
mind ‚Äì sometimes a small phrase added to a prompt can get dramatically better
replies, and providing examples helps. While large language models can
demonstrate surprisingly good reasoning skills, their intelligence is very
different than how our brains work. Prompt engineering aims to bridge the gap
and identify the right way to ask questions to get the best responses.</p>

<p>Nowadays there are many repositories of tried and tested prompts online. We
already saw a superprompt from <a href="https://github.com/f/awesome-chatgpt-prompts">https://github.com/f/awesome-chatgpt-prompts</a>.
Wolfram has a prompt repository at
<a href="https://resources.wolframcloud.com/PromptRepository/">https://resources.wolframcloud.com/PromptRepository/</a>. Many other prompt
repositories are available on GitHub.</p>

<p>As we just saw, coming up with a good prompt is hard work. In fact, from
personal experience, I can say that 80% of lighting up a scenario involving a
large language model is coming up with the right prompt (one of the reasons
<em>prompt engineering</em> is emerging as a new discipline). So, what do we do once we
find the perfect prompt?</p>

<h2 id="prompt-templates">Prompt templates</h2>

<p>Once we do come up with the perfect prompt, we want to store this somewhere and
reuse it. We can parameterize the context (instance-specific information) based
on the scenario while maintaining the common part of the prompt.</p>

<p>Let‚Äôs implement a simple templating system: we‚Äôll store the template as a JSON
file. This should contain a prompt. The customizable parts we put around <code>{{}}</code>,
for example <code>{{action}}</code> is customizable and we replace it with some actual
value when we make a call to OpenAI.</p>

<p>We can optionally set other parameters, like we discussed in chapter 2 (e.g.
<code>n</code>, <code>max_tokens</code>, <code>temperature</code>, <code>stop</code>).</p>

<p>Listing 3.12 shows an example of templating a prompt that can help us with some legal work. Let‚Äôs say after careful tuning, we realized we get the best results if we explicitly tell the large language model it is a lawyer, and lower the temperature. We can store this in <code>lawyer.json</code>.</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;max_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;You are a lawyer. {{action}}: {{document}}&quot;</span>
<span class="p">}</span>
</pre></div>

<p><em>Listing 3.12: Lawyer prompt template.</em></p>

<p>In this case we customize <code>temperature</code> (lower since we want some help with
legal documents and we don‚Äôt want the model to get too creative) and
<code>max_tokens</code> (since we‚Äôre playing with the API while learning and don‚Äôt want to
pay too much for running code samples). Our prompt tells the model to act as a
lawyer and has two customizable parts: an <code>action</code> and a <code>document</code>.</p>

<p>Figure 3.1 shows how prompt templating works.</p>

<div class="footnotes">
<hr>
<ol>

<li id="fn1">
<p>From <a href="https://github.com/f/awesome-chatgpt-prompts">https://github.com/f/awesome-chatgpt-prompts</a>.&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

<li id="fn2">
<p>See <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a>.&nbsp;<a href="#fnref2" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>

</article>
<nav>
<p>

<span>Previous: <a href="large-language-models.html">Large Language Models</a>. 


</p>
</nav>
<footer><span>By <a href="https://vladris.com/">Vlad Ri»ôcu»õia</a>&nbsp;|&nbsp;<a href="https://github.com/vladris/llm-book/issues/new">üì£ Feedback</a>&nbsp;|&nbsp;<a href="https://tinyletter.com/vladris">‚úâÔ∏è Subscribe</a></span></footer>
</body>
</html>