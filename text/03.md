# Prompt Engineering

![Prompt Engineering](images/03/cover.png)

In this chapter:

* Designing powerful prompts.
* Using templates for prompts.
* Selecting between multiple templates.
* Chaining prompts to perform complex tasks.

This chapter is all about prompts, the fundamental way we interact with large
language models. Prompts are key components of any solution built around these
models, so we need to have a solid understanding of how to leverage them to the
maximum.

We’ll start with *prompt design* and *tuning* and see how small tweaks to the
prompt can elicit very different responses from a large language model. We’ll
iterate over a simple example and look at some tips on what we should include in
a prompt.

If we’ll spend so much time coming up with the perfect prompt, we’ll probably
want to reuse it. We’ll see how we can do this via *prompt templating*. We’ll
implement a generic function that will enable us to specify prompt templates as
JSON files, then load these to fill in the prompts we send to the model.

In many scenarios, we have multiple prompt templates we can use, and we need to
solve the problem of which one to pick based on what the user wants. Luckily,
understanding user intent is something large language models are great at. We’ll
implement *prompt selection* and automate picking the right prompt.

Most non-trivial tasks can’t be accomplished through a single prompt – we’ll
have to divide and conquer. We can complete complex tasks with multiple
(connected) prompts, something known as *prompt chaining*. We’ll cover the
arguably simplest type of chaining – maintaining a chat history, then we’ll look
at a more complex scenario involving a multi-step workflow. Let’s start with the
basics though: design.

## Prompt design & tuning

In chapter 1 we briefly touched on the emerging discipline of *prompt
engineering* and defined it as the process of creating and optimizing natural
language prompts for large language models. Let’s see why prompts are such a key
piece of integrating these models into software solutions.

We’ll start with a simple scenario: we have some content we’d like summarized.
You can use <https://chat.openai.com/> for the examples in this section, or the
OpenAI playground (<https://platform.openai.com/playground/>). Listing 3.1 shows
the content – a business plan for a new minimalist phone.

```text
Our company aims to disrupt the smartphone market by introducing a minimalist
phone that focuses on the essentials. We believe that in a world where
technology is becoming increasingly complex, there is a growing demand for a
device that simplifies our lives and allows us to be more present in the moment.

Our minimalist phone will have a simple and sleek design, with a monochrome
display and limited features. It will focus on the core functionalities of a
phone, such as calling, texting, and taking photos, while eliminating
unnecessary distractions and time-wasting apps. By doing so, our phone will
promote mindfulness and productivity, while also reducing screen time and
digital overload.

In terms of pricing, we plan to position our minimalist phone as a premium
product that is worth the investment. We believe that our target audience, which
consists of busy professionals, digital minimalists, and people who value
simplicity and mindfulness, will be willing to pay a premium for a device that
aligns with their values and lifestyle.

To market our minimalist phone, we will leverage social media and influencer
marketing, as well as targeted advertising campaigns. We will also partner with
like-minded brands and organizations that share our vision of simplifying and
improving people's lives. Finally, we will prioritize customer satisfaction and
offer exceptional customer service and support, as well as regular updates and
improvements to the phone's software and functionality.

Overall, we are confident that our minimalist phone will revolutionize the
smartphone industry and offer a refreshing alternative to the current state of
technology. We look forward to launching our product and making a positive
impact on people's lives.
```

*Listing 3.1: A business plan for a minimalist phone (AI generated).*

We’d like to get this summarized into a few short bullet points, which we can
surface in our UI. We prompt the model with a simple prompt shown in listing 3.2
(business plan omitted).

```text
Summarize this business plan "<business plan from listing 3.1 goes here>"
```

*Listing 3.2: Prompt to summarize the business plan.*

We might get back something like listing 3.3 (again, large language models are
non-deterministic so you might get a different response).

```text
The business plan outlines a strategy to disrupt the smartphone market by
introducing a minimalist phone that focuses on the essentials, with a simple and
sleek design, limited features, and a monochrome display. The phone will be
positioned as a premium product, targeting busy professionals, digital
minimalists, and people who value simplicity and mindfulness. The company plans
to leverage social media, influencer marketing, and partnerships with
like-minded brands to market the phone. The company will prioritize customer
satisfaction, offer regular updates and improvements, and aims to make a
positive impact on people's lives.
```

*Listing 3.3: Summary of business plan.*

This is a good summary, but not quite what we want – let’s explicitly ask for bullet points. Listing 3.4 shows the *refined prompt*.

```text
Summarize this business plan "<business plan from listing 3.1 goes here>" in a
few short bullet points
```

*Listing 3.4: Refined prompt to summarize the business plan.*

Since we introduced the term, let’s also define it:

> **Definition**: A *refined prompt* is a prompt revised and optimized through
multiple iterations to elicit the most accurate and desirable responses from a
large language model. This typically involves tuning the language, syntax,
structure, and context of the prompt to make it more specific and effective in
achieving the desired outcome.

Refined prompts are crucial for improving the performance of large language
models across all applications. In our case, the refined prompt gives us the
output in listing 3.5.

```text
* Company aims to disrupt smartphone market with minimalist phone
* Phone will have simple and sleek design, with limited features
* Focus on core functionalities of calling, texting, and photos
* Target audience consists of busy professionals, digital minimalists, and those
  who value simplicity and mindfulness
* Phone will be positioned as a premium product worth investment
* Marketing will leverage social media, influencer marketing, and targeted
  advertising
* Partnership with like-minded brands and organizations
* Priority on customer satisfaction and support with regular updates and
  improvements
* Goal is to revolutionize smartphone industry and make positive impact on
  people's lives
```

*Listing 3.5: Summary of business plan.*

This is closer, but still too many bullet points and too many words – our UI can’t fit all of this. Let’s try again with listing 3.6.

```text
Summarize this business plan "<business plan from listing 3.1 goes here>" in no
more than 5 bullet points, at most 7 words each
```

*Listing 3.6: More detailed prompt.*

This gets us the output in listing 3.7.

```text
* Minimalist phone focusing on essentials
* Promotes mindfulness and productivity
* Premium product for busy professionals
* Leverage social media and partnerships
* Revolutionize smartphone industry with simplicity
```

*Listing 3.7: Summary of business plan.*

This is close to what we want. This simple example was an exercise in *prompt
design* and *tuning*.

> **Definition**: *Prompt design* is the process of constructing a high-quality
input sequence, or prompt, that will guide the model to generate a desired
output.

The prompt typically consists of a few words, phrases, or sentences that provide
context and direction for the model to generate text that aligns with the user's
intention. Effective prompt design significantly improves the quality and
relevance of the generated text.

Prompt design involves carefully selecting and arranging the words and phrases
in the prompt to guide the model toward the desired output, while avoiding
ambiguity or confusion.

We are still discovering the best, most effective prompt recipes, but a starting
point for a good prompt includes telling the model:

* **Who to act like** - This can be “you are an AI assistant”, or “respond as a
  high-school teacher”, or “act like Stephen King”.
* **Additional context** – We’ll cover this in a lot more depth later on when we
  discuss memory in chapter 5. For now, think of this as any additional
  information that helps clarify the ask or provides data that can be used when
  generating the answer.
* **Who the audience is** – You can tweak the response by being specific about
  the audience (e.g., “explain like I’m 5 years old”).
* **How the output should look like** – You can be as descriptive as needed.
  *“Short summary”, “500 word article”, or “5 bullet points no more than 7 words
  *each”.
* **How to go about generating the output** – Additional instructions for the
  *large language model, like “think step by step”.
* **Examples** – It helps to give examples; we’ll talk more about this in the
  next chapter. Examples are good way of showing the large language model what
  you expect.
* **Proper syntax** – Using the correct capitalization, grammar, and punctuation
  *helps (as it should more closely match data the model has been trained on).

For example, if we’re talking about integration within other systems, we care
about the shape of the output. Instead of asking `summarize this business plan`,
we are specific about how we want the format (`5 bullet points, no more than 7
words each`).

Of course, for complex scenarios, you won’t get the prompt right in one go.
You’ll have to refine it several times. This iterative process is called *prompt
tuning*.

> **Definition**: *Prompt tuning* refers to the iterative process of refining
and adjusting prompts used with large language models. In prompt tuning, the
goal is to create prompts that are optimized to produce the desired output from
the model.

People are getting more and more interesting outputs out of large language
models by coming up with intricate prompts. These highly tuned prompts are
colloquially called *superprompts*.

> **Sidebar: Superprompt example – interview practice**[^1]
>
> An example of one of this type of “magical” prompt is a prompt that enables
you to practice interviewing with ChatGPT. In the ChatGPT UI, you can start
with:
>
> `I want you to act as an interviewer. I will be the candidate and you will ask
me the interview questions for the {{position}} position. I want you to only
reply as the interviewer. Do not write all the conversation at once. I want you
to only do the interview with me. Ask me the questions and wait for my answers.
Do not write explanations. Ask me the questions one by one like an interviewer
does and wait for my answers. My first sentence is "Hi".`
>
> Replace `{{position}}` with the actual position you want to practice for, then
you can use ChatGPT to help you prepare!
>
> Note the detailed description of how the model should reply and what it should
not output. This prompt was likely created with a lot of tuning, starting from a
simpler ask then iterating as the model replied in unrealistic ways. For
example, the “do not write all the conversation at once” probably ended up there
as without it the large language model would imagine a whole interview exchange
and respond with it in one go.

A somewhat magical phrase that seems to help in many prompts is “think step by
step” or variations of it, like “think about this logically”. This is called
*chain-of-thought*, explicitly asking the large language model to list the steps
it takes to arrive at a result rather than just producing the result.

### Chain-of-thought prompts

For example, for more complex questions that involve multiple steps, the model
might offer a wrong response without additional priming. The prompt in listing
3.8, when run against `text-davinci-003`, gets the wrong response.

```text
In a bouquet of 12 flowers, half are roses. Half of the roses are red. How many
red roses are in the bouquet?
```

*Listing 3.8: Multiple-step problem prompt.*

The response is `6 roses`, which is wrong. `gpt-3.5-turbo` seems to answer this
correctly, but you get the point – more convoluted questions could elicit a
wrong response. Interestingly enough, if we enhance our prompt with “think step
by step” as in listing 3.9, the model produces the correct response.

```text
In a bouquet of 12 flowers, half are roses. Half of the roses are red. How many
red roses are in the bouquet? Think step by step.
```

*Listing 3.9: Chain-of-thought prompt.*

Now we get a more detailed, step-by-step answer, shown in listing 3.10.

```text
Step 1: Half of 12 flowers is 6. 
Step 2: Half of 6 roses is 3.
Answer: There are 3 red roses in the bouquet.
```

*Listing 3.10: Chain-of-thought prompt response.*

Turns out the model could generate the correct response all along, but it needed
additional guidance. In some cases, simply adding “think step by step” or
similar phrases is enough. In other cases, including an example of how this
step-by-step reasoning looks like in the prompt helps. See Listing 3.11 for an
example of this.

```text
Q: In a bouquet of 24 flowers, half are roses. A third of the roses are red. How
many red roses are in the bouquet?

A: The bouquet has 24 flowers, so half of them being roses means there are 12
roses. A third of the roses are red out of 12 rose, so there are 4 red roses in
the bouquet.

Q: In a bouquet of 12 flowers, half are roses. Half of the roses are red. How
may red roses are in the bouquet?

A:
```

*Listing 3.11: Chain-of-thought prompt with examples.*

When using this prompt with `text-davinci-003`, the response mimics the example
and arrives at the correct answer.

> **Definition**: *Chain-of-thought prompting* is a technique that can be used
to improve the reasoning and accuracy performance of large language models by
providing rationales for a given word or phrase. It improves the reasoning
ability of large language models by prompting them to generate a series of
intermediate steps that lead to the final answer of a multi-step problem.

Chain-of-though explicitly encourages the large language model to generate
intermediate rationales for solving a problem, either through a phrase like
“think step by step” or by providing a series of reasoning steps in a
demonstration that is part of the prompt.

In general, chain-of-thought prompting can elicit better reasoning from large
language models on logic, math, and symbolic reasoning tasks[^2]. Keep this in
mind – sometimes a small phrase added to a prompt can get dramatically better
replies, and providing examples helps. While large language models can
demonstrate surprisingly good reasoning skills, their intelligence is very
different than how our brains work. Prompt engineering aims to bridge the gap
and identify the right way to ask questions to get the best responses.

Nowadays there are many repositories of tried and tested prompts online. We
already saw a superprompt from <https://github.com/f/awesome-chatgpt-prompts>.
Wolfram has a prompt repository at
<https://resources.wolframcloud.com/PromptRepository/>. Many other prompt
repositories are available on GitHub.

As we just saw, coming up with a good prompt is hard work. In fact, from
personal experience, I can say that 80% of lighting up a scenario involving a
large language model is coming up with the right prompt (one of the reasons
*prompt engineering* is emerging as a new discipline). So, what do we do once we
find the perfect prompt?

## Prompt templates

Once we do come up with the perfect prompt, we want to store this somewhere and
reuse it. We can parameterize the context (instance-specific information) based
on the scenario while maintaining the common part of the prompt.

Let’s implement a simple templating system: we’ll store the template as a JSON
file. This should contain a prompt. The customizable parts we put around `{{}}`,
for example `{{action}}` is customizable and we replace it with some actual
value when we make a call to OpenAI.

We can optionally set other parameters, like we discussed in chapter 2 (e.g.
`n`, `max_tokens`, `temperature`, `stop`).

Listing 3.12 shows an example of templating a prompt that can help us with some legal work. Let’s say after careful tuning, we realized we get the best results if we explicitly tell the large language model it is a lawyer, and lower the temperature. We can store this in `lawyer.json`.

```json
{
    "temperature": 0.2,
    "max_tokens": 100,
    "prompt": "You are a lawyer. {{action}}: {{document}}"
}
```

*Listing 3.12: Lawyer prompt template.*

In this case we customize `temperature` (lower since we want some help with
legal documents and we don’t want the model to get too creative) and
`max_tokens` (since we’re playing with the API while learning and don’t want to
pay too much for running code samples). Our prompt tells the model to act as a
lawyer and has two customizable parts: an `action` and a `document`.

Figure 3.1 shows how prompt templating works.

[^1]: From <https://github.com/f/awesome-chatgpt-prompts>.
[^2]: See <https://arxiv.org/abs/2201.11903>.
