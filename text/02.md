# Large Language Models

![Large Language Models](images/02/cover.png)

In this chapter:

* Understanding large language models.
* Navigating the OpenAI model offerings.
* Counting tokens.
* Calling the OpenAI API.

We won’t cover the science behind large language models since this book focuses
on integrating these pre-trained models into broader software solutions. That
said, we’re going to quickly touch upon what it means (and how expensive it is!)
to train such a model. Once trained, we can use these models in our solutions.

OpenAI offers several families of models, including GPT-3.5, GPT-4, and DALL·E.
Each family contains one or more models. Some we’ll just mention, some we’ll use
intensely throughout the book.

A key concept to understand is that of tokens. Large language models represent
text slightly different than natural language – instead of words, they use
tokens. Each model has some token limit (maximum number of tokens it can
process) and OpenAI charges API calls based on token usage. We’ll learn about
tokens, limits, and how to get from text to tokens.

We’ll also have an in-depth look at the OpenAI API – we’ll first look at all the
components of a response, then we’ll see some of the available request
parameters. We’ll go over some scenarios and code for when we would use each of
these parameters. Let’s start with the overview of large language models.

## Large language models

Large language models are machine learning models that use deep neural networks
to generate human-like language output, based on patterns learned from vast
amounts of text. These models can perform various natural language processing
(NLP) tasks, including language generation, machine translation, question
answering, and sentiment analysis, among others.

The recent breakthrough in large language models was the development of
transformer-based architectures, such as GPT (Generative Pre-trained
Transformer) and BERT (Bidirectional Encoder Representations from Transformers).
These models can learn from massive amounts of data, often referred to as
"pre-training". They can then be fine-tuned for specific tasks.

Large language models have seen a significant increase in performance and
capabilities in recent years, with models such as GPT-3 and, more recently,
GPT-4 achieving remarkable results on a wide range of benchmarks.

![Figure 1](./images/02/fig1.png)

*Figure 2.1: Training, fine tuning, and prompting.*

Figure 2.1 shows *training*, *fine tuning*, and *prompting* (we’ll cover each in
more detail below). Training is a one-time, expensive ordeal, dealing with huge
amounts of data. Fine-tuning can optionally adjust a model to some specific
domain. This involves much smaller volumes of data. Finally, the model is used
by users prompting it.

### Training

Large language models are trained on massive amounts of text data. This process
involves feeding the model with huge datasets of text, such as books, articles,
and websites. The model then uses this data to learn patterns and relationships
between words, ultimately creating a vast network of connections between them.
This enables the model to generate human-like responses to queries or prompts.

> **Sidebar: Reinforcement Learning from Human**
>
> Large language models are trained using *Reinforcement Learning from Human
Feedback* (RLHF). This is an approach combining reinforcement learning (RL) with
input and guidance from human experts. It aims to improve the learning process
of an RL agent by leveraging human knowledge and expertise to guide its
training.
>
> In traditional RL, an agent learns through trial and error by interacting with
an environment and receiving rewards or penalties based on its actions. However,
this learning process can be time-consuming and inefficient, especially in
complex and high-dimensional environments.
>
> RLHF introduces human feedback into the RL loop to provide additional
information to the agent. This feedback can take different forms, such as
explicit reward signals, demonstrations, or evaluations from human experts. By
incorporating human guidance, RLHF can accelerate the learning process and make
it more effective.
>
> One key thing to keep in mind is RLHF effectively builds some specialization
into the model, which goes beyond just predicting text.

Training large language models is an extremely expensive process. For instance,
training ChatGPT, one of the largest language models to date, is estimated to
have cost around $5 million. This is due to the enormous computational resources
required to process such vast amounts of data. The training process can take
weeks or even months to complete, and typically involves hundreds or thousands
of GPUs working in parallel.

One of the key features of large language models is the sheer number of
*parameters* they have.

> **Definition**: *Parameters* are the values or weights in the neural network
that are determined during the training process. These parameters dictate how
the model responds to different inputs and are crucial to the model's ability to
generate coherent and meaningful responses. 

Large language models can have billions of parameters, far more than other AI
models used in different applications.

Unlike other AI models that are updated over time, large language models are
typically considered "off-the-shelf" tools. This means that once they are
trained, they are fixed and cannot be retrained. However, they can be fine-tuned
for specific tasks, such as language translation or sentiment analysis, by
adjusting a subset of their parameters. This makes them highly versatile and
useful for a wide range of applications.

### Tuning

While re-training a large language model is not feasible, these models can be
*fine-tuned*.

> **Definition**: *Fine-tuning* is a process that involves taking a pre-trained
language model, such as ChatGPT, and adapting it to a specific domain or task.
This is achieved by training the model on a smaller, domain-specific dataset
that is relevant to the task at hand. Fine-tuning allows the model to learn the
nuances and specificities of a particular domain, enabling it to generate more
accurate and contextually appropriate responses.

For example, if you wanted to create a chatbot for a customer service platform,
you could fine-tune a pre-trained language model by feeding it with a dataset of
customer queries and their corresponding responses. The model would then use
this data to learn how to respond to similar queries in a way that is consistent
with the brand's tone and style. This would result in a more effective and
personalized customer service experience.

The scale of fine-tuning is much smaller than that of full training. This is
because the model has already been trained on a vast corpus of general language
data, and only needs to be further trained on a smaller, domain-specific
dataset. This makes fine-tuning a much quicker and more cost-effective process
than full training.

Furthermore, fine-tuning allows for more flexibility and adaptability in the use
of language models. Instead of having to train a new model from scratch for each
specific task, fine-tuning allows for the reuse of pre-existing models, saving
both time and resources. It also enables the development of highly specialized
models that are tailored to the specific needs of a particular domain or
application.

Overall, fine-tuning is a powerful tool that allows for the customization and
optimization of pre-trained language models. By feeding them with
domain-specific data, we can create highly effective and contextually
appropriate models that are well-suited to a wide range of applications.

### Prompting

Once a language model has been trained or fine-tuned, users can interact with it
by providing *prompts*, which are essentially questions or statements that are
inputted into the model. We defined prompting in chapter 1 as the starting input
given to a model in order to generate output.

Prompting allows users to extract specific information or generate responses
from the model, based on the input provided. For instance, a user might prompt a
language model with a sentence and expect it to generate a coherent and
grammatical response. Alternatively, they might prompt the model with a specific
question or query and expect it to provide an accurate and relevant answer.

The key difference between prompting a language model and traditional
programming is that language models are probabilistic, rather than
deterministic. This means that instead of producing a single, predetermined
output based on a given input, language models generate a range of possible
outputs based on statistical probabilities.

Prompting a language model requires a different mindset than coding for a
deterministic computer. It involves understanding how language models work and
being able to formulate prompts in a way that will elicit the desired response.
This requires an understanding of the model's strengths and limitations, as well
as an ability to formulate prompts that are precise and specific.

You might find it strange that we are comparing prompting with coding, but we
are effectively trying to leverage large language models embedded in software
systems, so the analogy is warranted – this is the new paradigm of computing
enabled by AI.

In this book, we will delve more deeply into the mechanics of prompting,
exploring different strategies and techniques for optimizing language model
performance. We will also discuss best practices for fine-tuning models,
selecting the right data, and evaluating model performance. Ultimately, our goal
is to harness the power of large language models and use them to solve
real-world problems in a variety of domains.

### Non-determinism and hallucinations

One of the key differences between traditional software development and working
with large language models is the issue of *determinism*. Traditional software
development involves writing code that produces a deterministic output for a
given input. In other words, if the same input is provided to the software, it
should always produce the same output.

However, large language models are probabilistic and non-deterministic. The same
input can generate different responses. This non-deterministic nature of large
language models can be both a strength and a weakness. On one hand, it allows
the model to generate a wide range of responses, which can be useful for
applications such as natural language generation, dialogue systems, and
chatbots. On the other hand, it also means that the model can produce
unpredictable or incorrect responses, which can be problematic in certain
situations.

One of the challenges with large language models is that they can sometimes
*hallucinate*.

> **Definition**: *Hallucinations* are generated responses that are not based on
the input or the training data. This can occur when the model lacks the
necessary information to generate an accurate response or when it makes
assumptions based on incomplete or incorrect data.

We should always keep in mind the probabilistic and non-deterministic nature of
large language models and be prepared for a range of possible responses. It’s
also important to carefully evaluate the output of the model to ensure its
accuracy and relevance. This requires a combination of technical expertise and
domain-specific knowledge, as well as an ability to adapt to the unique
challenges and opportunities presented by large language models.

In summary, the probabilistic and non-deterministic nature of large language
models is a key point to consider when interacting with them. This is very
different from traditional software development and requires a different mindset
and approach to ensure the best possible outcomes.

We talked about large language models, what it takes to train one, fine-tuning,
prompting, and the key gotcha: hallucinations. We’ll cover these in-depth over
the next few chapters, with plenty of code examples but this overview should
help us get started with using the models offered by OpenAI.

## OpenAI models

We can use the OpenAI API to retrieve the list of available models. Assuming you
got set up in chapter 1 with an API key and installed the `openai` Python
package, you should be able to run the code in listing 2.1.

```python
import openai
import os

openai.api_key = os.getenv("OPENAI_API_KEY")
models = openai.Model.list().data
for model in models:
    print(model.id)
```

*Listing 2.1: Retrieve available OpenAI models and print them.*

First, we get the `OPENAI_API_KEY` from the environment as usual. Calling
`Model.list()` gives us back the list of available models. There’s quite a few
of them. Let’s see what they’re about.

### Model families

For the most up to date model list, you can visit
<https://platform.openai.com/docs/models/overview>.

At the time of writing, *GPT-4* is in limited beta. Throughout this book, we
will focus on models that output natural language or code. The first family of
models, now deprecated, aimed specifically at processing and generating code,
was *Codex*. The *GPT-3* family of models understand and generate natural
language. A newer and improved version of these is the *GPT-3.5* family of
models, which can handle both natural language and code. *GPT-4* is the latest
in this series, in limited beta at the time of writing.

There are also models focused on *moderation* and *embeddings*, which we’ll
visit later in the book.

OpenAI also offer natural language-to-image (*DALL·E*) and audio-to-text
(*Whisper*) models, but we’re not going to focus on these. The full list
provided by OpenAI at the time of writing is listed in table 2.1.

| MODELS | DESCRIPTION |
| --- | --- |
| GPT-4 (in limited beta) | A set of models that improve on GPT-3.5 and can understand as well as generate natural language or code |
| GPT-3.5 | A set of models that improve on GPT-3 and can understand as well as generate natural language or code |
| DALL·E (in beta) | A model that can generate and edit images given a natural language prompt |
| Whisper (in beta) | A model that can convert audio into text |
| Embeddings | A set of models that can convert text into a numerical form |
| Moderation | A fine-tuned model that can detect whether text may be sensitive or unsafe |
| GPT-3 | A set of models that can understand and generate natural language |
| Codex (deprecated) | A set of models that can understand and generate code, including translating natural language to code |

*Table 2.1: OpenAI model families.*

### Completion models

We will focus on the so-called *completion models* and their applications.

> **Definition**: A *completion model* takes a prompt and returns one or more
predicted completions, meaning text that is most likely to follow the prompt.

In chapter 1, we prompted `text-davinci-003` (a model from the GPT-3.5
generation) to say “Hello world!” in Python. Let’s take another look at the code
(listing 2.2), since we’re trying to better understand the API.

```python
import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY')
response = openai.Completion.create(
    model='text-davinci-003',
    prompt='Say "Hello world" in Python')

print(response.choices[0].text)
```

*Listing 2.2: “Hello world” in Python by text-davinci-003.*

We are calling the create completion API, passing it the model and prompt. Since
chat/interactive interactions have become a key scenario, OpenAI introduced
*chat completion models*.

> **Definition**: *chat completion models* are very similar to completion
models, but optimized for chat conversations, with a slightly different API:
they expect input to be in the form of messages in a chat and the response is
also formatted as a chat message.

Chat completion models expect as input a series of messages in the format `role`
(which can be `system`, `user`, or `assistant`) and `message` (which is the
content of the message). Let’s get a chat completion model, `gpt-3.5-turbo`, to
say “Hello world!” in Python (listing 2.3).

```python
import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY')
response = openai.ChatCompletion.create(
    model='gpt-3.5-turbo',
    messages=[
        {'role': 'user', 'content': 'Say "Hello world" in Python'}
    ])

print(response.choices[0].message)
```

*Listing 2.3: “Hello world” in Python by gpt-3.5-turbo.*

Note that instead of a prompt parameter, we have a list of messages. In our
case, we pass as input a single message, from the user, with our desired prompt.

The response should look like listing 2.4.

```json
{
  "content": "print(\"Hello world\")",
  "role": "assistant"
}
```

*Listing 2.4: gpt-3.5-turbo response.*

Following the chat messages format, instead of just text, we now get back an
object representing the message, which includes the `role` (the model responds
as the `assistant`) and the `content` of the message.

At this point you might be wondering why there’s a need for this special class
of chat completion models.

### Why chat completion models

We will talk a lot more about memory in this book, but for now one key thing to
keep in mind is that the models we’re interacting with don’t have any memory
whatsoever. If you played with the interactive ChatGPT, you might think that it
does, as it follows the conversation and you can refer to previous messages, but
in fact this is not the case. Each request contains the full conversation
history!

For interactive scenarios, when using a completion model like
`text-davinci-003`, you would use a prompt like listing 2.5.

```text
You are a helpful AI assistant.

User: {{previous message}}
Assistant: {{previous response}}
User: {{new message}}
Assistant:
```

*Listing 2.5: Prompt template for a completion model simulating chat.*

Repeat the User/Assistant part for as many messages there are in the chat. Note
we start the prompt by telling the model how it should act like (`You are a
helpful AI assistant`), then follow up with a dialogue ending where we want the
model to start the completion.

For example, if the user wanted to prompt the model to use Ruby instead, we
would use the prompt in listing 2.6.

```text
You are a helpful AI assistant.

User: Say "Hello world" in Python
Assistant: print("Hello world")
User: Now do it in Ruby
Assistant:
```

*Listing 2.6: Prompt for a completion model simulating chat.*

A chat completion model prompt represents this in a more structured manner, as
show in listing 2.7.

```python
messages=[
    {'role': 'system', 'content': 'You are a helpful AI assistant'},
    {'role': 'user', 'content': 'Say "Hello world" in Python'},
    {'role': 'assistant', 'content': 'print("Hello world")'},
    {'role': 'user', 'content': 'Now do it in Ruby'}
]
```

*Listing 2.7: Prompt for a chat completion model.*

Note we would construct the list of actual messages at runtime based on what the
user asks and what the model replies, and we would pass it to the model on each
subsequent call. To the user it would seem the model “remembers” the
conversation - for example the second message in the chat doesn’t mention “Hello
world” anywhere, but the model will correctly output the Ruby version of “Hello
world”, `puts "Hello world"`. In fact, we are sending back the chat history on
the new call.

We saw a bunch of small hand-crafted calls to the models. At this point, we know
enough to implement a simple command line chat. Listing 2.8 shows our first
ChatGPT.

```python
import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY')
history = []

while True:
    prompt = input('user: ')
    if prompt == 'exit':
        break

    history.append({'role': 'user', 'content': prompt})
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo',
        messages=history)

    message = response.choices[0].message

    print(f'{message.role}: {message.content}')
    history.append({'role': message.role, 'content': message.content})
```

*Listing 2.8: Command line chat application.*

In this example, we maintain a history of the chat in the `history` variable,
which starts as empty. We request input from the user. We exit the loop if the
user types `exit`, otherwise we append the message to the `history` and call the
chat completion API.

We extract the first `message`, print it to the console, and append it to
history for subsequent calls.

That’s it! You can take it for a spin and have a quick chat with
`gpt-3.5-turbo`. Note if we would’ve used `text-davinci-003` for this, we
would’ve handcrafted the history by appending to a string.

The full list of GPT-3.5 models offered at the time of writing is captured in
table 2.2.

| MODEL | DESCRIPTION |
| --- | --- |
| gpt-3.5-turbo | Most capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration. |
| gpt-3.5-turbo-16k | Same capabilities as the standard gpt-3.5-turbo model but with 4 times the context. |
| gpt-3.5-turbo-0613 | Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released. |
| gpt-3.5-turbo-16k-0613 | Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released. |
| text-davinci-003 | Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports inserting completions within text. |
| text-davinci-002 | Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning. |
| code-davinci-002 | Optimized for code-completion tasks. |

*Table 2.2: GPT-3.5 models.*

ada, babbage, and currie mentioned in the table (`text-ada-001`,
`text-babbage-001`, and `text-currie-001`), together with `text-davinci-001`,
are part of the older GPT-3 family of models.

We’ll use `text-davinci-003` and `gpt-3.5-turbo` throughout this book. Table 2.2
mentions that `gpt-3.5-turbo` is 10 times cheaper than `text-davinci-003`. We
started with `text-davinci-003` for its simpler prompt structure, but since
`gpt-3.5-turbo` is the model that will continue being updated and it costs an
order of magnitude less, we will be using this model in most of the following
code examples.

Since we’re talking about costs, to better understand how OpenAI charges for
usage, we need to understand tokens.

## Tokens

Large language models don’t use exactly the words we pass in. A preprocessing
step converts natural language into *tokens*. The models “understand” tokens,
this processed representation. This helps models handle multiple languages,
formats, and reduce computational costs.

> **Definition**: A *token* refers to a sequence of characters or symbols that
is considered as a single unit of input to the model. Tokens can be individual
words, punctuation marks, numbers, or other types of symbols, and they are used
to break down a piece of text into smaller, more manageable units for processing
by the model.

Understanding what tokens are is very important for two main reasons: large
language models have limits on how many tokens they can process during a call
and OpenAI charges their API calls based on number of tokens used.

OpenAI uses *Byte-Pair Encoding* for its models. The algorithm works by
iteratively merging the most frequently occurring pair of bytes in the input
data into a single byte, until a predefined number of merge operations have been
performed or a target vocabulary size is reached. The different models available
have different vocabulary sizes.

For the English language, a token is about 3/4 of a word, so 100 tokens
represent around 75 words. OpenAI offers an online tokenizer at
<https://platform.openai.com/tokenizer> and, for programmatic use, the
`tiktoken` library: <https://github.com/openai/tiktoken>.

For example, our prompt `Say "Hello world" in Python` is 7 tokens-long: `Say`,
`·"`, `Hello`, `·world`, `"`, `·in`, and `·Python` (note tokenization
includes spaces and punctuation). Figure 2.2 shows an even nicer example, where
tokenization is not quite one-to-one with the text.

![Figure 2](images/02/fig2.png)

*Figure 2.2: Example of tokenization using the OpenAI online tokenizer. Each
highlight represents a token.*

Note, for example, that “accepting” is split into the tokens `accept` and `ing`.
“GPT-4” is split into 4 different tokens.

Let’s also see how we can do this programmatically.

### Using tiktoken

First, we need to install `tiktoken` (listing 2.9).

```sh
pip install tiktoken
```

*Listing 2.9: Installing tiktoken.*

Next, let’s encode a string and see how the model interprets it. Listing 2.10
shows the code to do this.

```python
import tiktoken

enc = tiktoken.encoding_for_model('text-davinci-003')
tokens = enc.encode('Say "Hello world" in Python')

print(tokens)
print([enc.decode_single_token_bytes(token) for token in tokens])
```

*Listing 2.10: Tokenizing text and viewing the tokens.*

First, we get the right encoding for the model we are using. We then encode the
phrase `Say "Hello world" in Python`. We print the encoded tokens, then we print
them decoded back into strings using the `decode_single_token_bytes()` function.

This should print the output in listing 2.11.

```text
[25515, 366, 15496, 995, 1, 287, 11361]
[b'Say', b' "', b'Hello', b' world', b'"', b' in', b' Python']
```

*Listing 2.11: tiktoken output.*

The first line is the tokens as they are fed into the large language model – as numbers. The second line prints their decoded representation, so we can see which characters ended up in which token.

At the time of writing, `tiktoken` supports 3 encodings:

1. `cl100k_base` for models `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`.
2. `p50k_base` for models `text-davinci-002` and `text-davinci-003`.
3. `r50k_base` for the GPT-3 family of models.

We can retrieve an encoder by name using the `get_encoding()` function (for
example, `tiktoken.get_encoding('p50k_base')`) or, to make sure we get the right
encoding for the model we are using, we can call the `encoding_for_model()`
function like we did in listing 2.10.

As we saw at the beginning of this section, the reason we need to understand
tokens, and potentially write code to parse text into tokens, is that sooner or
later we will run into token limits.

### Limits

